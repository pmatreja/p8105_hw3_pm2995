---
title: "p8105_hw3_pm2995"
author: "Priyal"
date: "10/8/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

library(tidyverse)
library(readxl)
library(p8105.datasets)
library(ggplot2)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```


#Problem 1

##Problem 1.1

Loading the data and some cleaning 

First we load the BRFSS data from the `p8105.datasets` package.

```{r}
data(brfss_smart2010)
```

Next, we do some cleaning to ensure appropriate variable names. I cleaned the data using janitor:: clean_names function converting them all to lower snake case, filter to include only overall health topic data and include responses from excellent to poor. Further, I organized the responses as a factor taking levels ordered from excellent to poor using mutate.

```{r tidy_brfss}
brfss_tidy = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  rename(location_abbr = locationabbr, location_desc = locationdesc) %>%
  mutate(response = as.factor(response)) %>% 
  mutate(
    response = fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))
    )
```


Specific Questions

##Problem 1.2

In 2002, which states were observed at 7 locations?

Answer:
The code chunk below filters to include the rows from year 2002 and groups the data by location_abbr (which includes states). Further, using summarize number of distinct locations in each state has been taken out. I have used filter to keep states that were observed at 7 locations.

```{r}
brfss_tidy %>% 
  filter(year == 2002) %>% 
  group_by(location_abbr) %>% 
  summarize(n_locations = n_distinct(location_desc)) %>% 
  filter(n_locations == 7)
```

Three states were observed at seven locations: CT, FL, and NC.

##Problem 1.3

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

Answer:
The code below groups the brfss_tidy data by location_abbr(state) and year and further uses summarize to take out the number of corresponding observations. It then creates a spaghetti plot that shows the number of locations in each state from 2002 to 2010. 


```{r}
brfss_tidy %>% 
  group_by(location_abbr, year) %>% 
  summarize(n_obs = n()) %>% 
  ggplot(aes(x = year, y = n_obs, color = location_abbr)) +
  geom_line() + labs(title = "No. of locations in each state from 2002 to 2010", x = "Year", y = "Number of observations") + theme(legend.position = "right")
```

There is a lot of clumping in this plot and peaks can be seen for the state of Florida for the year 2007 and 2010. Also, it can be seen that for NJ there were consistent number of locations from 2005 to 2006 and then from 2009 to 2010. 

#Problem 1.4

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

Answer:
The code below filters to keep the rows for the years 2002,2006 and 2010, Excellent response and for NY state. Further, it uses group_by function to group by the year and summarize to compute mean and standard deviations of for those years. The result is presented in the form of a table using knitr::kable.

```{r}
brfss_tidy %>%
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(response == "Excellent") %>% 
  filter(location_abbr == "NY") %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(data_value, na.rm = TRUE), sd_excellent = sd(data_value, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)
```

There is not much difference in the mean proportion for the excellent response for the years 2006 and 2010, whereas the mean proportion for the excellent response is a bit different in the year 2002.

#Problem 1.5

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

Answer:
The code below makes a five panel plot that shows the distribution of state level averages over time for each response category separately. To compute the average proportion in each category for each year and state, I have grouped the data by year, location_abbr(state) and response and further made the five panel plot by using facet grid to separate out each response category.


```{r}
brfss_tidy %>% 
  group_by(year,location_abbr,response) %>% 
  summarize(average = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = average, color = response)) + 
  geom_point() + 
  facet_grid(~ response) +
  labs(title = "Distribution of State Level Averages Over Time", x = "Year", y = "Mean Proportions") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
```

The distribution of state level averages over time suggest that the response "very good" was observed the most over the years and the response "poor" was observed the least.


#Problem 2

Loading the dataset 

Let's load the data from the `p8105.datasets` package. 

```{r}
data("instacart")
```
##Problem 2.1

Description of the dataset

There are `r nrow(instacart)` rows and `r ncol(instacart)` columns. All the variables of the dataset of class integer except eval_set, product_name, aisle, and department which are character. Let us take the example of `order_id` 1 that was placed by the `user_id` 112108.  For every product there is a  `product_id`, `product_name`. The product location is determined by four variables: `aisle_id`, `aisle`, `department_id` and `department`. Taking the example of yoghurt aisle; it has aisle_id 120, it belongs to dairy eggs department which has departemnt_id 16. In this case the total number of items that were added were 8; the order in which they were added can be seen from the `add_to_cart_order`(eg. if this field is 2 it means this is the 2nd item that was added to the cart). Out of these 8 items the items that are being reordered can be seen as `reordered` = 1. `order_number` denotes the order sequence of the user; in this case it is this user's 4th order with instacart. `order_dow` and `order_hour_of_day` give us the details about when that order was placed. For this customer, he placed the order on Thursday(considering 0 as sunday) at 10 am. `days_since_prior_order` gives the number of days that have past since the previous order by this user. If this is the first order for a customer, this field will be NA. For this particular customer, nine days have passes since he/she placed the last order. `eval_set` refers to which evaluation set this order belongs in. In this dataset all belong to train `eval_set`.


##Problem 2.2

How many aisles are there, and which aisles are the most items ordered from?

Answer:
The code chunks below will give the total number of aisles
```{r}
instacart %>% 
  distinct(aisle) %>% 
  nrow()
```
The total number of aisles are 134.

The code chunk below will arrange the aisles as per descending order of their count which will enable us to see the aisle from which the most items are ordered.
```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Most of the items are ordered from the fresh vegetables aisle followed by fresh fruits and packaged vegetable fruits. This totally makes sense as these aisles contain items that are an important ingredient of a meal and hence ordered the most.

##Problem 2.3 


##Problem 2.4
Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

Answer:

The code chunk below filters the specific popular aisles that are given in the question, groups them by aisle and product_name and filters to keep the product from each aisle that is ordered maximum number of times. The result is presented in the form of a table using knitr::kable.


```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_prod = n()) %>% 
  filter(n_prod == max(n_prod)) %>% 
  knitr::kable()
```

Little Brown Sugar(ordered 499 times) from the baking ingredients aisle, snacks sticks chicken & Rice Recipe Dog Treats(ordered 30 times) from the dog food care and organic baby spinach (ordered 9784 times) from the packaged vegetable fruits are the most popular items. Out of these three, organic baby spinach is ordered the maximum number of times which shows that the users ordering from Instacart are definitely going for a healthy choice.

##Problem 2.5

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean = round(mean(order_hour_of_day))) %>% 
  spread(key = order_dow, value = mean) %>%
  knitr::kable()

```

It appears that coffee ice cream is ordered mostly in afternoons. Only on a friday it is ordered a bit before. Pink lady apples are ordered mostly around noon. I have considered 0 as sunday for this dataset.


