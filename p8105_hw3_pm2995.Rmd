---
title: "p8105_hw3_pm2995"
author: "Priyal"
date: "10/8/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)

library(tidyverse)
library(readxl)
library(p8105.datasets)
library(ggplot2)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```


#Problem 1

##Problem 1.1

Loading the data and some cleaning 

First we load the BRFSS data from the `p8105.datasets` package.

```{r}
data(brfss_smart2010)
```

Next, we do some cleaning to ensure appropriate variable names. I cleaned the data using janitor:: clean_names function converting them all to lower snake case, filter to include only overall health topic data and include responses from excellent to poor. Further, I organized the responses as a factor taking levels ordered from excellent to poor using mutate.

```{r tidy_brfss}
brfss_tidy = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  rename(location_abbr = locationabbr, location_desc = locationdesc) %>%
  mutate(response = as.factor(response)) %>% 
  mutate(
    response = fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))
    )
```


Specific Questions

##Problem 1.2

In 2002, which states were observed at 7 locations?

Answer:
The code chunk below filters to include the rows from year 2002 and groups the data by location_abbr (which includes states). Further, using summarize number of distinct locations in each state has been taken out. I have used filter to keep states that were observed at 7 locations.

```{r}
brfss_tidy %>% 
  filter(year == 2002) %>% 
  group_by(location_abbr) %>% 
  summarize(n_locations = n_distinct(location_desc)) %>% 
  filter(n_locations == 7)
```

In 2002, three states were observed at seven locations: CT, FL, and NC.

##Problem 1.3

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

Answer:
The code below groups the brfss_tidy data by location_abbr(state) and year and further uses summarize to take out the number of corresponding observations. It then creates a spaghetti plot that shows the number of locations in each state from 2002 to 2010. 

```{r}
brfss_tidy %>% 
  group_by(year, location_abbr) %>% 
  distinct(location_desc) %>% 
  summarize(n_obs = n()) %>%
  ggplot(aes(x = year, y = n_obs, color = location_abbr)) + geom_point() +
  geom_line() + labs(title = "No. of locations in each state from 2002 to 2010", x = "Year", y = "Number of distinct locations") + theme(legend.position = "right")
```


There is a lot of clumping in this plot. For most of the states the number of distict locations that were observed is 10 or less from 2002 to 2010. There is some amount of clumping in 10-15 region as well which is more towards years 2007-2010. Peaks can be seen for the state of Florida for the year 2007(44) and 2010(41). For NJ we see constant number of locations/counties were observed for 2005 to 2006 (19) and then from 2009 to 2010(19). As compared to other states, there were higher number of counties observed for NJ in 2007(16) and 2008(18). For the year 2008, there were 16 locations observed in NC and for 2010 there were 18 locations observed in TX. These are some of the distinctly visible outliers from the spaghetti plot. 

#Problem 1.4

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

Answer:
The code below filters to keep the rows for the years 2002,2006 and 2010, Excellent response and for NY state. Further, it uses group_by function to group by the year and summarize to compute mean and standard deviations of for those years. The result is presented in the form of a table using knitr::kable.

```{r}
brfss_tidy %>%
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(response == "Excellent") %>% 
  filter(location_abbr == "NY") %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(data_value, na.rm = TRUE), sd_excellent = sd(data_value, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)
```

There is not much difference in the mean proportion for the excellent response for the years 2006 and 2010, whereas the mean proportion for the excellent response is a bit different in the year 2002.

#Problem 1.5

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

Answer:
The code below makes a five panel plot that shows the distribution of state level averages over time for each response category separately. To compute the average proportion in each category for each year and state, I have grouped the data by year, location_abbr(state) and response and further made the five panel boxplot by using facet grid to separate out each response category.


```{r}
brfss_tidy %>% 
  group_by(year, location_abbr,response) %>% 
  summarize(average = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = average, group = year, color = response)) + 
  geom_boxplot() + 
  facet_grid(~ response) +
  labs(title = "Distribution of State Level Averages Over Time", x = "Year", y = "Mean Proportions") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
```

The distribution of state level averages over time suggest that the response "very good" was observed the most over the years and the response "poor" was observed the least.


#Problem 2

Loading the dataset 

Let's load the data from the `p8105.datasets` package. 

```{r}
data("instacart")
```
##Problem 2.1

Description of the dataset

There are `r nrow(instacart)` rows and `r ncol(instacart)` columns. Each row is indicating distict item of an order. All the variables of the dataset of class integer except eval_set, product_name, aisle, and department which are character. There are total `r instacart %>% distinct(order_id) %>% nrow` distinct orders in this dataset. Let us take the example of `order_id` 1 that was placed by the `user_id` 112108.  For every product there is a  `product_id`, `product_name`. The product location is determined by four variables: `aisle_id`, `aisle`, `department_id` and `department`. Taking the example of yoghurt aisle; it has aisle_id 120, it belongs to dairy eggs department which has departemnt_id 16. In this case the total number of items that were added were 8; the order in which they were added can be seen from the `add_to_cart_order`(eg. if this field is 2 it means this is the 2nd item that was added to the cart). Out of these 8 items the items that are being reordered can be seen as `reordered` = 1. `order_number` denotes the order sequence of the user; in this case it is this user's 4th order with instacart. `order_dow` and `order_hour_of_day` give us the details about when that order was placed. For this customer, he placed the order on Thursday(considering 0 as sunday) at 10 am. `days_since_prior_order` gives the number of days that have past since the previous order by this user. If this is the first order for a customer, this field will be NA. For this particular customer, nine days have passes since he/she placed the last order. `eval_set` refers to which evaluation set this order belongs in. In this dataset all belong to train `eval_set`.


##Problem 2.2

How many aisles are there, and which aisles are the most items ordered from?

Answer:
The code chunks below will give the total number of aisles.

```{r}
instacart %>% 
  distinct(aisle) %>% 
  nrow()
```
The total number of aisles are 134.


The code chunk below will arrange the aisles as per descending order of their count which will enable us to see the aisle from which the most items are ordered.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Most of the items are ordered from the fresh vegetables aisle followed by fresh fruits and packaged vegetable fruits. This totally makes sense as these aisles contain items that are an important ingredient of a meal and hence ordered the most.

##Problem 2.3 

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

Answer:

The code chunk below groups the instacart data by aisle and uses summarize to count the number of ordered items per aisle. It further uses mutate to reorder the aisles based on number of ordered items in descending order and makes a scatterplot showing number of items ordered in each aisle. I have tried to improve the readability of the plot by tilting the x axis text by ninety degrees. 

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>% 
  mutate(aisle = reorder(aisle, desc(n_items))) %>% 
  ggplot(aes(x = aisle, y = n_items)) + geom_point() + labs(title = "Number of items ordered from each aisle", x = "Aisle", y = "No. of items")  + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), text = element_text(size = 8))
```

Most number of items are ordered from fresh vegetables aisle followed by fresh fruits, packaged vegetable fruits, yoghurt and packaged cheese.

To improve the readability and for more clarity, I have made another plot where I have just included the top ten aisles from which most items are ordered. I did this by filtering the top ten most ordered items using min_rank function.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>%
  mutate(aisle = reorder(aisle, desc(n_items))) %>% 
  filter(min_rank(aisle) < 11) %>% 
  
  ggplot(aes(x = aisle, y = n_items)) + geom_point() + labs(title = "Number of items ordered from each aisle", x = "Aisle", y = "No. of items")  + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


##Problem 2.4
Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

Answer:

The code chunk below filters the specific popular aisles that are given in the question, groups them by aisle and product_name and filters to keep the product from each aisle that is ordered maximum number of times. The result is presented in the form of a table using knitr::kable.


```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_prod = n()) %>% 
  filter(n_prod == max(n_prod)) %>% 
  knitr::kable()
```

Little Brown Sugar(ordered 499 times) from the baking ingredients aisle, snacks sticks chicken & Rice Recipe Dog Treats(ordered 30 times) from the dog food care and organic baby spinach (ordered 9784 times) from the packaged vegetable fruits are the most popular items. Out of these three, organic baby spinach is ordered the maximum number of times which shows that the users ordering from Instacart are definitely going for a healthy choice.

##Problem 2.5

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

Answer:
The code chunk below first filters to keep the product names specified in the question. Further, it groups the data by product names and order_dow (which is day of the week when the order was placed) to summarize the mean of order_hour_of_day. This gives us the mean of order_hour_of_day for each product. I have used spread with key = order_dow to spread out(long to wide format) the values of the computed mean for each corresponding day. 
 
```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(order_dow = ordered(order_dow, levels = c(0:6), labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean = round(mean(order_hour_of_day))) %>% 
  spread(key = order_dow, value = mean) %>% 
  knitr::kable()
```

It appears that coffee ice cream is ordered mostly in afternoons. Only on a friday it is ordered a bit early. Pink lady apples are ordered mostly around noon. I have considered 0 as sunday for this dataset.

#Problem 3

Loading the dataset

```{r}
data("ny_noaa")
```

##Problem 3.1

Short description of the dataset

The NY NOAA data set is super extensive dataset with `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The key variables include id, tmax and tmin which are class `character`, prcp, snow , snwd are class `integer` and date. The variable ID denotes weather station ID, date of observation is included under date, precipitation (in tenths of mm) under prcp, snowfall observations are included in snow (in mm), depth of snow is inculded in snwd, tmax and tmin represent the maximum and minimum temperatures (in tenths of degrees C). The data include weather information for various weather station across the NY state from January 1, 1981 through December 31, 2010. It has been mentioned on the website with the data that each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data. There are total 
`r sum(is.na(ny_noaa$tmax))` values missing for tmax, that denotes
`r {sum(is.na(ny_noaa$tmax)) / (ny_noaa %>% nrow)} * 100`% values are missing for tmax. Also, `r sum(is.na(ny_noaa$tmin))` values are missing for tmin, that denotes `r {sum(is.na(ny_noaa$tmin)) / (ny_noaa %>% nrow)} * 100`% values missing for tmin indicating that not all stations recorded maximum and minimum temperatures for all the days. There are total `r sum(is.na(ny_noaa$prcp))` number of values missing for precipitation which indicates `r {sum(is.na(ny_noaa$prcp)) / (ny_noaa %>% nrow)}*100`% of values are missing for precipitation. This indicates that not all stations record precipitation on a daily basis.

Taking the snow variable into account, the total missing values are `r sum(is.na(ny_noaa$snow))` which indicates `r {sum(is.na(ny_noaa$snow))/(ny_noaa %>% nrow)} * 100`% values are missing. For snow depth, `r sum(is.na(ny_noaa$snwd))` values that is `r {sum(is.na(ny_noaa$snwd))/(ny_noaa %>% nrow)} * 100`% values are missing. 

We should be aware about these missing data while filtering values or while looking at the summary statistics of the data. Overall, there are maximum values (43.71 %) values missing for the tmax and tmin because of which we will be unable to see the whole picture about the temperature. 

##Problem 3.2

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. 

Answer:

The below code chunk is for tidying up the ny noaa dataset. I have split the date variable into year, month and day by using the separate function. Since variables maximum temperature and minimum temperature were in tenths of degrees celsius, I have divided them by 10. I have done the same for precipitation as the values were in one tenths of mm and now all the variables seem have reasonable units. The variable snow has its values in mm which is reasonable.

```{r}
ny_noaa_tidy = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    prcp = prcp/10, 
    tmax = as.numeric(tmax)/10, 
    tmin = as.numeric(tmin)/10
    ) 
  
```


For snowfall, what are the most commonly observed values? Why?

```{r}
ny_noaa_tidy %>% 
  count(snow) %>% 
  arrange(desc(n))
```

Zero is the most commonly observed value for snowfall because it did not snow most of the days at various weather stations across NY state.


##Problem 3.3

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

Answer:

The code chunk below makes a two panel spaghetti plot showing the average max temperature in January and in July in each station across years. I used the tidy version of the ny_noaa data here and used filter to keep the rows for the month of January and July. I mutated the month variable to include month name values using the month.name variable which is built in R. Further, I used group by function to group the data by id(which denotes station), year and month and used summarize to generate the average tmax across stations. As there were lots of missing values, I used na.rm = TRUE to exclude NA values before computation. Further, ggplot was used to create the spaghetti plots and facetting approach was used to create the 2 panel plot.

```{r}

ny_noaa_tidy %>% 
  filter(month %in% c("01","07")) %>%
  mutate(month = month.name[as.numeric(month)]) %>%
  group_by(id, year, month) %>%
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) + 
  geom_point() +
  geom_line() +
  facet_grid(~ month) +
  labs(title = "Average Maximum Temperature at Stations in January and July (1981-2010)", 
    x = "Year", 
    y = "Average Maximum Temperature (ºC)") +
  theme(axis.text.x = element_text(angle = 90)) + theme(legend.position = "none") 

```

The plot shows that the mean maximum temperatures across all the stations in NY across 1981-2010 was much greater in July than January. There is a lot of clumping in these plots. It was around 20-34ºC in July and -10 to 10ºC in January for most of the stations across New York. There are some outliers for the month of January; In 1982 the average maximum temperature dropped to around -13ºC, in 2004 there was a drop to around -11ºC, and in 2005 it dropped to -12ºC across a station in NY. There are few outliers for the month of July; it dropped to around 14ºC(13.95 actually) in 1988, 18ºC in 2004 and was around 19ºC in 1984, 2000 and 2007 across one of the station in NY. 

